{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "from surprise import Reader\n",
    "from surprise import Dataset\n",
    "from surprise.model_selection import cross_validate\n",
    "from surprise import NormalPredictor\n",
    "from surprise import KNNBasic\n",
    "from surprise import KNNWithMeans\n",
    "from surprise import KNNWithZScore\n",
    "from surprise import KNNBaseline\n",
    "from surprise import SVD\n",
    "from surprise import BaselineOnly\n",
    "from surprise import SVDpp\n",
    "from surprise import NMF\n",
    "from surprise import SlopeOne\n",
    "from surprise import CoClustering\n",
    "from surprise.accuracy import rmse\n",
    "from surprise import accuracy\n",
    "from surprise.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "import recmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_movies = pd.read_csv('C:/Users/saleh/Desktop/SM/data movielens/ml-25m/movies.csv')\n",
    "raw_ratings = pd.read_csv('C:/Users/saleh/Desktop/SM/data movielens/ml-25m/ratings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(movies,ratings, movie_threshold):\n",
    "    '''\n",
    "    Function for processing the movies and rating data set. \n",
    "    It removes duplicates (if any), and outputs the final ratings dataframe.\n",
    "    It removes any movies with less than the threshold number of ratings.\n",
    "    '''\n",
    "\n",
    "    raw_movies = movies\n",
    "    \n",
    "    if raw_movies.movieId.nunique()  != raw_movies.title.nunique():\n",
    "        print('There are some duplicate titles')\n",
    "        print('\\n')\n",
    "        print('Removing the following duplicate titles \\n')\n",
    "        v = raw_movies.title.value_counts()\n",
    "        for i,movie in enumerate(list(raw_movies[raw_movies.title.isin(v.index[v.gt(1)])].title.unique())):\n",
    "            print('{0}. {1}'.format(i+1,movie))\n",
    "         \n",
    "    else:\n",
    "        print('No duplicate titles')\n",
    "        processed_movies = raw_movies\n",
    "        \n",
    "        \n",
    "    # check if any user has rated the same movie more than once\n",
    "    print('\\n')\n",
    "    if ratings.duplicated(subset=['userId','movieId']).any():\n",
    "        print('There are duplicate ratings for same movies by some users')\n",
    "    else:\n",
    "        print('There are no duplicate ratings')\n",
    "        \n",
    "    # Remove duplicate movie Ids from ratings data\n",
    "    s = raw_movies.title.value_counts()\n",
    "    dup_movies = list(raw_movies[raw_movies.title.isin(s.index[s.gt(1)])].movieId.unique())\n",
    "    ratings1 = ratings.drop(ratings[ratings.movieId.isin(dup_movies)].index)\n",
    "    \n",
    "    # Filter data for movies with atleast 10 ratings\n",
    "\n",
    "    c = Counter(ratings1.movieId)\n",
    "    relevant_items = [k for k, count in c.items() if count >= movie_threshold]\n",
    "\n",
    "    print(np.shape(relevant_items))\n",
    "    filtered_ratings = ratings1.loc[ratings1.movieId.isin(relevant_items),:]\n",
    "    \n",
    "    print('\\n')\n",
    "    print('Total Movies = {}'.format(movies.movieId.nunique()))\n",
    "    print('Filtered Movies = {}'.format(filtered_ratings.movieId.nunique()))\n",
    "    \n",
    "    \n",
    "    return filtered_ratings[['userId','movieId','rating']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are some duplicate titles\n",
      "\n",
      "\n",
      "Removing the following duplicate titles \n",
      "\n",
      "1. Aladdin (1992)\n",
      "2. Men with Guns (1997)\n",
      "3. Dracula (1931)\n",
      "4. Saturn 3 (1980)\n",
      "5. Gossip (2000)\n",
      "6. Hamlet (2000)\n",
      "7. Clockstoppers (2002)\n",
      "8. Confessions of a Dangerous Mind (2002)\n",
      "9. Lagaan: Once Upon a Time in India (2001)\n",
      "10. Hostage (2005)\n",
      "11. Eros (2004)\n",
      "12. Enron: The Smartest Guys in the Room (2005)\n",
      "13. War of the Worlds (2005)\n",
      "14. Casanova (2005)\n",
      "15. Little Man (2006)\n",
      "16. Chaos (2005)\n",
      "17. Journey to the Center of the Earth (2008)\n",
      "18. Noise (2007)\n",
      "19. Family Life (1971)\n",
      "20. Blackout (2007)\n",
      "21. Berlin Calling (2008)\n",
      "22. Seven Years Bad Luck (1921)\n",
      "23. 9 (2009)\n",
      "24. Frozen (2010)\n",
      "25. Home (2008)\n",
      "26. Weekend (2011)\n",
      "27. Beneath (2013)\n",
      "28. Clear History (2013)\n",
      "29. Good People (2014)\n",
      "30. Paradise (2013)\n",
      "31. Crush (2009)\n",
      "32. Deranged (2012)\n",
      "33. Ecstasy (2011)\n",
      "34. Grace (2014)\n",
      "35. The Reunion (2011)\n",
      "36. Rebirth (2011)\n",
      "37. Simple Things (2007)\n",
      "38. Slow Burn (2000)\n",
      "39. Delirium (2014)\n",
      "40. Eden (2014)\n",
      "41. The Plague (2006)\n",
      "42. The Connection (2014)\n",
      "43. Black Field (2009)\n",
      "44. Inside (2012)\n",
      "45. Rose (2011)\n",
      "46. Holiday (2014)\n",
      "47. Macbeth (2015)\n",
      "48. Absolution (2015)\n",
      "49. Tag (2015)\n",
      "50. The Dream Team (2012)\n",
      "51. Another World (2014)\n",
      "52. An Inspector Calls (2015)\n",
      "53. Operator (2015)\n",
      "54. Truth (2015)\n",
      "55. The Tunnel (1933)\n",
      "56. Escalation (1968)\n",
      "57. Shelter (2015)\n",
      "58. Stranded (2015)\n",
      "59. The Forest (2016)\n",
      "60. Forsaken (2016)\n",
      "61. Camino (2016)\n",
      "62. Interrogation (2016)\n",
      "63. The Boss (2016)\n",
      "64. The Break-In (2016)\n",
      "65. The Midnight Man (2016)\n",
      "66. Savage (2011)\n",
      "67. Office (2015)\n",
      "68. Apparition (2014)\n",
      "69. The Promise (2016)\n",
      "70. Free Fall (2014)\n",
      "71. Sing (2016)\n",
      "72. Detour (2017)\n",
      "73. Classmates (2016)\n",
      "74. The Void (2016)\n",
      "75. Home (2016)\n",
      "76. Ava (2017)\n",
      "77. Blockbuster (2017)\n",
      "78. Suckerfish (1999)\n",
      "79. Escape Room (2017)\n",
      "80. Lucky (2017)\n",
      "81. Veronica (2017)\n",
      "82. Blood Money (2017)\n",
      "83. Let There Be Light (2017)\n",
      "84. Alone (2017)\n",
      "85. Haunted (2017)\n",
      "86. Contact (1992)\n",
      "87. Cold War (2018)\n",
      "88. Cargo (2017)\n",
      "89. Delirium (2018)\n",
      "90. Feral (2018)\n",
      "91. Reset (2017)\n",
      "92. Believer (2018)\n",
      "93. The Angel (2018)\n",
      "94. Lost & Found (2018)\n",
      "95. I See You (2019)\n",
      "96. Beats (2019)\n",
      "97. The Lonely Island Presents: The Unauthorized Bash Brothers Experience (2019)\n",
      "98. American Woman (2019)\n",
      "\n",
      "\n",
      "There are no duplicate ratings\n",
      "(24247,)\n",
      "\n",
      "\n",
      "Total Movies = 62423\n",
      "Filtered Movies = 24247\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>296</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>306</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>307</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>665</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>899</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  movieId  rating\n",
       "0       1      296     5.0\n",
       "1       1      306     3.5\n",
       "2       1      307     5.0\n",
       "3       1      665     5.0\n",
       "4       1      899     3.5"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings = process_data(raw_movies,raw_ratings,10)\n",
    "ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24812557, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data into a surprise dataframe from pandas\n",
    "reader = Reader(rating_scale=(-1, 5))\n",
    "data = Dataset.load_from_df(ratings, reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " <surprise.prediction_algorithms.matrix_factorization.SVD object at 0x000001F724205780> Done! \n",
      "\n",
      "\n",
      " <surprise.prediction_algorithms.random_pred.NormalPredictor object at 0x000001F724205B70> Done! \n",
      "\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "\n",
      " <surprise.prediction_algorithms.knns.KNNWithZScore object at 0x000001F724205908> Done! \n",
      "\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "\n",
      " <surprise.prediction_algorithms.knns.KNNWithMeans object at 0x000001F724205978> Done! \n",
      "\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "\n",
      " <surprise.prediction_algorithms.baseline_only.BaselineOnly object at 0x000001F724205940> Done! \n",
      "\n",
      "\n",
      " <surprise.prediction_algorithms.slope_one.SlopeOne object at 0x000001F7242058D0> Done! \n",
      "\n",
      "\n",
      " <surprise.prediction_algorithms.matrix_factorization.NMF object at 0x000001F7242059B0> Done! \n",
      "\n",
      "\n",
      " <surprise.prediction_algorithms.co_clustering.CoClustering object at 0x000001F7242059E8> Done! \n",
      "\n"
     ]
    }
   ],
   "source": [
    "benchmark = []\n",
    "# Iterate over all algorithms\n",
    "for algorithm in [SVD(), NormalPredictor(), KNNWithZScore(), KNNWithMeans(), BaselineOnly(), SlopeOne(), NMF(),CoClustering()]:\n",
    "    # Perform cross validation\n",
    "    results = cross_validate(algorithm, data, measures=['RMSE'], cv=4)\n",
    "    print('\\n {} Done! \\n'.format(algorithm))\n",
    "    # Get results & append algorithm name\n",
    "    tmp = pd.DataFrame.from_dict(results).mean(axis=0)\n",
    "    tmp = tmp.append(pd.Series([str(algorithm).split(' ')[0].split('.')[-1]], index=['Algorithm']))\n",
    "    benchmark.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_rmse</th>\n",
       "      <th>fit_time</th>\n",
       "      <th>test_time</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Algorithm</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SVD</th>\n",
       "      <td>0.851633</td>\n",
       "      <td>6.674611</td>\n",
       "      <td>0.276563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BaselineOnly</th>\n",
       "      <td>0.852446</td>\n",
       "      <td>0.187035</td>\n",
       "      <td>0.198443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNNWithZScore</th>\n",
       "      <td>0.855247</td>\n",
       "      <td>0.451507</td>\n",
       "      <td>4.098663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNNWithMeans</th>\n",
       "      <td>0.856230</td>\n",
       "      <td>0.379080</td>\n",
       "      <td>3.605330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SlopeOne</th>\n",
       "      <td>0.857689</td>\n",
       "      <td>1.912584</td>\n",
       "      <td>8.994021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NMF</th>\n",
       "      <td>0.881182</td>\n",
       "      <td>7.401268</td>\n",
       "      <td>0.256774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CoClustering</th>\n",
       "      <td>0.901045</td>\n",
       "      <td>3.108642</td>\n",
       "      <td>0.309699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NormalPredictor</th>\n",
       "      <td>1.392487</td>\n",
       "      <td>0.171441</td>\n",
       "      <td>0.240363</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 test_rmse  fit_time  test_time\n",
       "Algorithm                                      \n",
       "SVD               0.851633  6.674611   0.276563\n",
       "BaselineOnly      0.852446  0.187035   0.198443\n",
       "KNNWithZScore     0.855247  0.451507   4.098663\n",
       "KNNWithMeans      0.856230  0.379080   3.605330\n",
       "SlopeOne          0.857689  1.912584   8.994021\n",
       "NMF               0.881182  7.401268   0.256774\n",
       "CoClustering      0.901045  3.108642   0.309699\n",
       "NormalPredictor   1.392487  0.171441   0.240363"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "benchmark\n",
    "pd.DataFrame(benchmark).set_index('Algorithm').sort_values('test_rmse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using ALS\n"
     ]
    }
   ],
   "source": [
    "model = SVD(n_factors= 100,n_epochs= 20,lr_all= 0.005,reg_all= 0.02)\n",
    "results = cross_validate(model, data, measures=['RMSE'], cv=4, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8524706135102402\n",
      "7.311672270298004\n",
      "0.29663753509521484\n"
     ]
    }
   ],
   "source": [
    "#results.keys()\n",
    "print(np.mean(results['test_rmse']))\n",
    "print(np.mean(results['fit_time']))\n",
    "print(np.mean(results['test_time']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd_options = {'n_factors': [50,100,200],\n",
    "               'n_epochs': [10,20,30],\n",
    "               'lr_all': [0.0025,0.005,0.0075],\n",
    "               'reg_all': [0.01,0.02,0.03]\n",
    "               }\n",
    "\n",
    "gs = GridSearchCV(SVD, param_grid=svd_options, measures=[\"rmse\"], cv=4)\n",
    "\n",
    "gs.fit(data)\n",
    "\n",
    "gs_results = pd.DataFrame(gs.cv_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_factors': 100, 'n_epochs': 20, 'lr_all': 0.0075, 'reg_all': 0.03}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#gs_results.sort_values('rank_test_rmse')\n",
    "\n",
    "gs.best_params[\"rmse\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#print('Using ALS')\n",
    "#bsl_options = {'method': 'als',\n",
    "#               'n_epochs': 10,\n",
    "#               'reg_u': 15,\n",
    "#               'reg_i': 10\n",
    "#               }\n",
    "#model = BaselineOnly(bsl_options=bsl_options)\n",
    "#cross_validate(model, data, measures=['RMSE'], cv=4, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset, testset = train_test_split(data, test_size=0.25)\n",
    "model = SVD(lr_all=0.0075, reg_all=0.03)\n",
    "predictions = model.fit(trainset).test(testset)\n",
    "accuracy.rmse(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Iu(uid):\n",
    "    \"\"\" return the number of items rated by given user\n",
    "    args: \n",
    "      uid: the id of the user\n",
    "    returns: \n",
    "      the number of items rated by the user\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return len(trainset.ur[trainset.to_inner_uid(uid)])\n",
    "    except ValueError: # user was not part of the trainset\n",
    "        return 0\n",
    "    \n",
    "def get_Ui(iid):\n",
    "    \"\"\" return number of users that have rated given item\n",
    "    args:\n",
    "      iid: the raw id of the item\n",
    "    returns:\n",
    "      the number of users that have rated the item.\n",
    "    \"\"\"\n",
    "    try: \n",
    "        return len(trainset.ir[trainset.to_inner_iid(iid)])\n",
    "    except ValueError:\n",
    "        return 0\n",
    "    \n",
    "df = pd.DataFrame(predictions, columns=['userid', 'itemid', 'rating', 'prediction', 'details'])\n",
    "df['user_ratings'] = df.userid.apply(get_Iu)\n",
    "df['item_ratings'] = df.itemid.apply(get_Ui)\n",
    "df['error'] = abs(df.prediction - df.rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>itemid</th>\n",
       "      <th>rating</th>\n",
       "      <th>prediction</th>\n",
       "      <th>user_ratings</th>\n",
       "      <th>item_ratings</th>\n",
       "      <th>error</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>userid</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.321723</td>\n",
       "      <td>153</td>\n",
       "      <td>8</td>\n",
       "      <td>0.005890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>318</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.212689</td>\n",
       "      <td>20</td>\n",
       "      <td>23</td>\n",
       "      <td>0.279111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>527</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.504479</td>\n",
       "      <td>19</td>\n",
       "      <td>8</td>\n",
       "      <td>1.004479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>162</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.910559</td>\n",
       "      <td>138</td>\n",
       "      <td>6</td>\n",
       "      <td>0.164515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>36</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.153441</td>\n",
       "      <td>34</td>\n",
       "      <td>13</td>\n",
       "      <td>0.153441</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        itemid  rating  prediction  user_ratings  item_ratings     error\n",
       "userid                                                                  \n",
       "1            1     3.0    3.321723           153             8  0.005890\n",
       "2          318     3.0    3.212689            20            23  0.279111\n",
       "3          527     0.5    1.504479            19             8  1.004479\n",
       "4          162     1.0    2.910559           138             6  0.164515\n",
       "5           36     2.0    3.153441            34            13  0.153441"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()\n",
    "df.groupby('userid').min().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values('error',ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings[ratings['movieId'] == 2488]['rating'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings.loc[ratings['movieId'] == 2488]['rating'].hist()\n",
    "plt.xlabel('rating')\n",
    "plt.ylabel('Number of ratings')\n",
    "plt.title('Number of ratings Movie 2488 has received')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings.loc[ratings['userId'] == 594]['rating'].hist()\n",
    "plt.xlabel('rating')\n",
    "plt.ylabel('Number of ratings')\n",
    "plt.title('Number of ratings user 594 has given')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def all_recos(trainset,model):\n",
    "    '''Return the full list of user-item pairs not present in the training data along with the predicted ratings.\n",
    "    Args:\n",
    "        trainset generated from surprise data.\n",
    "        Model, which will be used for predictions.\n",
    "    Returns:\n",
    "    A list of recommendations [userid,itemid,prediction]\n",
    "    '''\n",
    "    recos = trainset.build_anti_testset()\n",
    "    \n",
    "    final_recos = []\n",
    "    \n",
    "    for uid,iid,_ in recos:\n",
    "        row = [uid,iid,model.predict(uid,iid)[3]]\n",
    "        final_recos.append(row)\n",
    "    \n",
    "    return final_recos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Function for getting top n predictions for each user from a set of predictions\n",
    "def get_top_n(predictions, n=10):\n",
    "    '''Return the top-N recommendation for each user from a set of predictions.\n",
    "    Args:\n",
    "        predictions(list of Prediction objects): The list of predictions, as\n",
    "            returned by the test method of an algorithm.\n",
    "        n(int): The number of recommendation to output for each user. Default\n",
    "            is 10.\n",
    "    Returns:\n",
    "    A dict where keys are user (raw) ids and values are lists of tuples:\n",
    "        [(raw item id, rating estimation), ...] of size n.\n",
    "    '''\n",
    "    \n",
    "    predictions.sort(key=lambda x: x[0], reverse=False)\n",
    "\n",
    "    # First map the predictions to each user.\n",
    "    top_n = defaultdict(list)\n",
    "    for uid, iid, est in predictions:\n",
    "        top_n[uid].append((iid, est))\n",
    "    \n",
    "    top_n_list = []\n",
    "\n",
    "    # Then sort the predictions for each user and retrieve the k highest ones.\n",
    "    for uid, user_ratings in top_n.items():\n",
    "        user_ratings.sort(key=lambda x: x[1], reverse=True)\n",
    "        top_n_list.append(user_ratings[:n])\n",
    "        top_n[uid] = user_ratings[:n]\n",
    "\n",
    "    return top_n_list, top_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for getting a sorted dictionary of top rated items from a user's testset\n",
    "def create_testset_dict(testset):\n",
    "    '''\n",
    "    Return the items rated by a given user within the testset, sorted in descending order.\n",
    "    \n",
    "    :params: testset: A testset generated via Surprise\n",
    "    : returns testset_dict: a collection of users, items which they rated and the given rating\n",
    "    '''\n",
    "    testset_dict = defaultdict(list)\n",
    "    \n",
    "    testset.sort(key=lambda x: x[0], reverse=False)\n",
    "    \n",
    "    for row in testset:\n",
    "        uid, iid, gt_rating = row\n",
    "        testset_dict[uid].append((iid, gt_rating))\n",
    "        \n",
    "    testset_list = []\n",
    "    for uid, user_ratings in testset_dict.items():\n",
    "        user_ratings.sort(key=lambda x: x[1], reverse=True)\n",
    "        testset_list.append(user_ratings)\n",
    "        testset_dict[uid] = user_ratings\n",
    "        \n",
    "    return testset_list, testset_dict\n",
    "\n",
    "#Function for getting all items from surprise trainset\n",
    "def all_trainset_items(trainset):\n",
    "    '''\n",
    "    Function for creating a list of all trainset items\n",
    "    params: trainset (surprise trainset object)\n",
    "    :returns all_items (list)\n",
    "    '''\n",
    "    all_items = [trainset.to_raw_iid(item) for item in trainset.all_items()]\n",
    "    return all_items\n",
    "\n",
    "\n",
    "#Function for getting all items from predictions\n",
    "def all_pred_items(recos):\n",
    "    '''\n",
    "    Function for creating a list of all trainset items\n",
    "    params: trainset (surprise trainset object)\n",
    "    :returns all_items (list)\n",
    "    '''\n",
    "    all_pred_items = list(set([iid for uid,iid,_ in recos]))\n",
    "    return all_pred_items\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def build_full_testset(surprise_data):\n",
    "    '''\n",
    "    Function for creating a full testset object from surprise data\n",
    "    params: surprise data (surprise dataset)\n",
    "    :returns full_test (a full testet)\n",
    "    '''\n",
    "    raw_ratings = surprise_data.raw_ratings\n",
    "    full_test = surprise_data.construct_testset(raw_ratings)\n",
    "    return full_test\n",
    "\n",
    "def get_mark(predictions, actuals, k):\n",
    "    '''\n",
    "    Function for calculating mean average recall @ k. \n",
    "    params: predictions (predicted item list), actuals (full item list), k (int)\n",
    "    :returns mean_ark (float)\n",
    "    '''\n",
    "    mean_ark = recmetrics.mark(actual=actuals, predicted=predictions, k=k)\n",
    "    return mean_ark\n",
    "\n",
    "#Function for getting coverage\n",
    "def get_coverage_score(predicted, catalog):\n",
    "    '''\n",
    "    Function for calculating coverage score. \n",
    "    params: predicted_items (predicted item list), trainset_items (full)\n",
    "    :returns cov (float)\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    predicted_flattened = [p[0] for sublist in predicted for p in sublist]\n",
    "    unique_predictions = len(set(predicted_flattened))\n",
    "    coverage = round(unique_predictions/(len(catalog)* 1.0)*100,2)\n",
    "    return coverage\n",
    "\n",
    "#Function for getting personalization score\n",
    "def get_personalization_score(predicted_items):\n",
    "    '''\n",
    "    Function for calculating personalization score.\n",
    "    params: predicted_items (predicted item list)\n",
    "    :returns pers (float)\n",
    "    '''\n",
    "    \n",
    "    pred_list = []\n",
    "\n",
    "    for sublist in predicted_items:\n",
    "        pred_list.append([i[0] for i in sublist])\n",
    "    \n",
    "    pers = recmetrics.personalization(pred_list)\n",
    "    return pers\n",
    "\n",
    "#Function for getting intra-list similarity score\n",
    "def get_intra_list_similarity(predicted_items,feature_df):\n",
    "    '''\n",
    "    Function for calculating intra-list similarity score\n",
    "    params: predicted_items (predicted item list)\n",
    "    :returns intra-list similarity score (float)\n",
    "    '''\n",
    "    \n",
    "    pred_list = []\n",
    "\n",
    "    for sublist in predicted_items:\n",
    "        pred_list.append([i[0] for i in sublist])\n",
    "    \n",
    "    pers = recmetrics.intra_list_similarity(pred_list,feature_df)\n",
    "    return pers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def apk(actual, predicted, k=10):\n",
    "    if len(predicted)>k:\n",
    "        predicted = predicted[:k]\n",
    "    \n",
    "    score = 0.0\n",
    "    num_hits = 0.0\n",
    "    for i,p in enumerate(predicted):\n",
    "        m, r = p\n",
    "        if m in list(m for m,r in actual) and m not in list(m for m,r in predicted[:i]):\n",
    "            #print('Yes')\n",
    "            num_hits += 1.0\n",
    "            score += num_hits / (i+1.0)\n",
    "    return score / min(len(actual),k)\n",
    "\n",
    "\n",
    "def mapk(actual, predicted, k=10):\n",
    "    \"\"\"\n",
    "    Computes the mean average precision at k.\n",
    "\n",
    "    This function computes the mean average prescision at k between two lists\n",
    "    of lists of items.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    actual : list\n",
    "             A list of lists of elements that are to be predicted \n",
    "             (order doesn't matter in the lists)\n",
    "    predicted : list\n",
    "                A list of lists of predicted elements\n",
    "                (order matters in the lists)\n",
    "    k : int, optional\n",
    "        The maximum number of predicted elements\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    score : double\n",
    "            The mean average precision at k over the input lists\n",
    "\n",
    "    \"\"\"\n",
    "    return np.mean([apk(a,p,k) for a,p in zip(actual, predicted)])\n",
    "    #return [apk(a,p,k) for a,p in zip(actual, predicted)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract year from movie title\n",
    "\n",
    "def get_item_feature_df(raw_movies):\n",
    "    if 'genres' not in raw_movies.columns:\n",
    "        raw_movies = pd.read_csv('C:/Users/saleh/Desktop/SM/Data/1M/ml-latest-small/movies.csv')\n",
    "\n",
    "    \n",
    "    # Perform one-hot encoding for genres and then drop the \"no genres listed column\"\n",
    "    d = pd.get_dummies(raw_movies.genres)\n",
    "\n",
    "    cols = [c for c in d.columns if '|' not in c]\n",
    "\n",
    "\n",
    "    for genre in cols:\n",
    "        raw_movies[genre] = raw_movies['genres'].str.contains(genre).astype(int)\n",
    "\n",
    "    processed_movies = raw_movies.drop(['title','genres'], axis=1)\n",
    "    processed_movies.set_index('movieId', inplace=True)\n",
    "    \n",
    "    return processed_movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_recommendations = all_recos(trainset,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saleh\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:15: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "#Creating Evaluation dict\n",
    "\n",
    "top_n_list, top_n_dict = get_top_n(full_recommendations, 10)\n",
    "testset_list, testset_dict = create_testset_dict(testset)\n",
    "feature_df = get_item_feature_df(raw_movies)\n",
    "trainset_items = all_trainset_items(trainset)\n",
    "processed_movies = get_item_feature_df(raw_movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from ml_metrics import average_precision\n",
    "#average_precision.mapk(actual=testset_list, predicted=top_n_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP@k: 0.04397814207650273\n",
      "Coverage Score: 15.58%\n",
      "Personalization Score: 88.85%\n",
      "Intra-list Similarity Score: 0.31\n"
     ]
    }
   ],
   "source": [
    "# Get Model Scores\n",
    "\n",
    "print('MAP@k: {}'.format(mapk(testset_list, top_n_list,5)))\n",
    "print('Coverage Score: {}%'.format(get_coverage_score(top_n_list, trainset_items)))\n",
    "print('Personalization Score: {}%'.format(round(get_personalization_score(top_n_list)*100,2)))\n",
    "print('Intra-list Similarity Score: {}'.format(round(get_intra_list_similarity(top_n_list,processed_movies),2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
